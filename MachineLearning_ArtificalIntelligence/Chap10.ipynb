{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: Dynamic Programming and Reinforcement Learning\n",
    "\n",
    "## 10.1 Introduction\n",
    "\n",
    "- The theory of dynamic programming ist typically attributed to Richard Bellman.\n",
    "- early stages of AI in the form of expert systems\n",
    "\n",
    "\n",
    "## 10.2 Fundamental Equation of Dynamic Programming\n",
    "\n",
    "- typically the problem can be stated in the form of a single equation, called *Bellman equation*\n",
    "- at each step there exists a state and a possible set of actions \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "V(s_t) = max_{a_t \\in \\Gamma(s_t)} (F(s_t, a_t) + \\beta V(T(s_t, a_t)))\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "## 10.3 Classes of Problems Under Dynamic Programming\n",
    "\n",
    "- Travleing Salesman Problem (TSP)\n",
    "- Quadratic optimization usn recursive least squares method (RLS)\n",
    "- Finding the shortest distance between two nodes in graph\n",
    "- Viterbi algorithm for solvinh HMM\n",
    "\n",
    "\n",
    "## 10.4 Reinforcement Learning\n",
    "\n",
    "The reinforcement learning framework is based on interaction between two primary entities: system and environment\n",
    "\n",
    "### 10.4.1 Characteristics of Reinforcement Learning\n",
    "\n",
    "- no preset labelled training data\n",
    "- action space is predefined\n",
    "- system chooses an action at every instance of time\n",
    "- at every instance of time, a feedback from the environment is rcorded (positive, negative, neutral)\n",
    "- can be delay in the feedback\n",
    "- learn while interacting with the environment\n",
    "- environment is not static\n",
    "- total training space is practically infinite\n",
    "- training and application phase are not separate\n",
    "\n",
    "\n",
    "### 10.4.2 Framework and Algorithm\n",
    "\n",
    "- reinforcement learning is a framework and not an algorithm\n",
    "\n",
    "\n",
    "## 10.5 Exploration and Exploitation\n",
    "\n",
    "- start of learning process: no prior knowledge\n",
    "$\\newline\\Rightarrow$ Exploration\n",
    "- when sufficient feedback is gathered the system starts to associate the actions and corresponding feedback\n",
    "- producing mor deterministic actions\n",
    "$\\newline\\Rightarrow$ Exploitation\n",
    "\n",
    "- Good tradeoff between these two necessary\n",
    "- system more biased towards exploitation $\\rightarrow$ likely to get stuck in local optimum\n",
    "\n",
    "\n",
    "## 10.6 Application of Reinforcement Learning\n",
    "\n",
    "- Chess programs\n",
    "- Robotics\n",
    "- Video games\n",
    "- Personalization\n",
    "\n",
    "\n",
    "## 10.7 Theory of Reinforcement Learning\n",
    "\n",
    "### 10.7.1 Variations in Learning\n",
    "\n",
    "1. Q-learning \n",
    "2. SARSA\n",
    "3. Monte Carlo\n",
    "\n",
    "#### 10.7.1.1 Q-Learning\n",
    "\n",
    "1. Initialize Q-table (all possible state and action combinations)\n",
    "2. Initialize $\\beta$\n",
    "3. Choose action, tradeoff between exploration and exploitation\n",
    "4. Perform action, measure reward\n",
    "5. Update corresponding q-value\n",
    "6. Update state\n",
    "7. Continue till targets are reached\n",
    "\n",
    "\n",
    "#### 10.7.1.2 Sarsa\n",
    "\n",
    "- *state-action-reward-state-action*\n",
    "- update to Q-learning\n",
    "\n",
    "\n",
    "## 10.8 Implementing Reinforcement Learning\n",
    "\n",
    "Not in sklearn avaiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Write reinforcement learning algorithm for iris, see book exercise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
